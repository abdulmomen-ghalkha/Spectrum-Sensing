{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DNN-FM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Darth-Kronos/Spectrum-Sensing/blob/master/DNN_FM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPo7c3pSVvr2",
        "cellView": "form"
      },
      "source": [
        "#@title Library  { form-width: \"30%\" }\n",
        "import tensorflow.keras as keras\n",
        "from scipy.stats import entropy\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.metrics import roc_curve\n",
        "import matplotlib.pyplot as plt \n",
        "import pandas as pd \n",
        "from scipy.fft import fft\n",
        "from scipy.stats import gennorm\n",
        "import numpy as np\n",
        "from scipy.special import gamma\n",
        "import math\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense,LSTM\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn import preprocessing\n",
        "from scipy.signal import savgol_filter\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Reshape, GlobalAveragePooling1D\n",
        "from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D\n",
        "from keras.utils import np_utils\n",
        "!pip install -q -U tensorflow-addons\n",
        "!pip install keras-tcn\n",
        "drive.mount('/content/drive')\n",
        "# from utils.utils import save_logs\n",
        "# from utils.utils import calculate_metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER0bD09qXuew",
        "cellView": "form"
      },
      "source": [
        "#@title DATA  { form-width: \"30%\" }\n",
        "def find_energy(signal, snr):\n",
        "  Y = fft(signal)\n",
        "  abs_Y = abs(Y)**2\n",
        "  r = np.mean(abs_Y)\n",
        "  return 2*snr*np.sum(r)\n",
        "\n",
        "def find_gp(signal):\n",
        "  return np.exp(np.mean(np.log(abs(np.array(signal)))))\n",
        "\n",
        "def find_lp(signal,p):\n",
        "  lp = np.mean(abs(signal)**p)\n",
        "  return lp \n",
        "\n",
        "def normalizeData(raw_data):\n",
        "  # print(np.var(raw_data))\n",
        "  data = (raw_data - np.mean(raw_data))/np.sqrt(np.var(raw_data))\n",
        "  return data\n",
        "\n",
        "def find_de(signal):\n",
        "    pdf,_ = np.histogram(signal)\n",
        "    pdf = pdf/len(signal)\n",
        "    return entropy(pdf,base=2)\n",
        "\n",
        "batch = 50\n",
        "df = pd.read_csv('Dataset/Dataset3.csv')\n",
        "raw_data = df.iloc[:,0]\n",
        "raw_data = raw_data.to_numpy()\n",
        "signal = normalizeData(raw_data[0:50000])\n",
        "lenSample = len(signal)\n",
        "# beta = 1.5\n",
        "TIME_PERIODS = 1\n",
        "def createTrainTest(data,Features):\n",
        "  x = data[:,0:Features]\n",
        "  y = data[:,-1]\n",
        "  return x,y\n",
        "\n",
        "def createFeature(beta,snrDB):\n",
        "  var = 1\n",
        "  p=0.5\n",
        "\n",
        "  snr = 10**(snrDB/10)\n",
        "  featuresMatrix = np.zeros(shape=(2000,5))\n",
        "  dfNoise = pd.read_csv('Dataset/FMNoise.csv',header=None,index_col=0)\n",
        "  noise = normalizeData(dfNoise.values.flatten())\n",
        "  \n",
        "  for i in range(0,lenSample,batch):\n",
        "    if i+batch <= lenSample:\n",
        "\n",
        "      noise_fm = np.random.choice(noise,batch)\n",
        "    \n",
        "      h1 =  noise_fm +  np.sqrt(snr)*signal[i:i+batch]\n",
        "      h0 = np.random.choice(noise,batch)\n",
        "      # features for h1\n",
        "      energy = find_energy(h1,snr)\n",
        "      de = find_de1(h1)\n",
        "      gp = find_gp(h1)\n",
        "      lp = find_lp(h1,p)\n",
        "      featuresMatrix[i//batch] = [gp,de,lp,energy,1]\n",
        "      \n",
        "      # features for h0\n",
        "      energy = find_energy(h0,snr)\n",
        "      de = find_de1(h0)\n",
        "      gp = find_gp(h0)\n",
        "      lp = find_lp(h0,p)\n",
        "      featuresMatrix[(lenSample+i)//batch] = [gp,de,lp,energy,0]\n",
        "      \n",
        "\n",
        "  featuresMatrix[:,0] = featuresMatrix[:,0]/max(featuresMatrix[:,0])\n",
        "  featuresMatrix[:,1] = featuresMatrix[:,1]/max(featuresMatrix[:,1])\n",
        "  featuresMatrix[:,2] = featuresMatrix[:,2]/max(featuresMatrix[:,2])\n",
        "  featuresMatrix[:,3] = featuresMatrix[:,3]/max(featuresMatrix[:,3])\n",
        "\n",
        "  np.random.shuffle(featuresMatrix)\n",
        "\n",
        "  return featuresMatrix\n",
        "  \n",
        "def findPd(pf,pd):\n",
        "  temp = np.where((pf<=0.1) & (pf>0.0))[0]\n",
        "  if temp.size>0 and pd[temp[-1]] != 0.0:\n",
        "    return pd[temp[-1]]\n",
        "  else:\n",
        "    return 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3c2uvG2VvtU"
      },
      "source": [
        "snrDB = np.flip(np.arange(-20,0.5,0.5))\n",
        "\n",
        "beta = 1\n",
        "\n",
        "pd_cnn = np.ones(len(snrDB))\n",
        "pd_mlp = np.ones(len(snrDB))\n",
        "pd_fcn = np.ones(len(snrDB))\n",
        "pd_rnet =np.ones(len(snrDB))\n",
        "pd_lstm =np.ones(len(snrDB))\n",
        "pd_tcn = np.ones(len(snrDB))\n",
        "\n",
        "num_time_periods = 1\n",
        "num_classes = 1\n",
        "num_sensors=4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cW93TQ49gKOF",
        "cellView": "form"
      },
      "source": [
        "#@title MLP  { form-width: \"30%\" }\n",
        "\n",
        "model_mlp = Sequential()\n",
        "model_mlp.add(keras.Input(shape=(num_sensors,)))\n",
        "\n",
        "model_mlp.add(Dense(100,activation='relu'))\n",
        "model_mlp.add(Dense(100,activation='relu'))\n",
        "model_mlp.add(Dense(100,activation='relu'))\n",
        "model_mlp.add(Dense(1,activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yu588FSeeeZF",
        "cellView": "form"
      },
      "source": [
        "#@title CNN  { form-width: \"30%\" }\n",
        "model_cnn = Sequential()\n",
        "model_cnn.add(Reshape((1, num_sensors), input_shape=(num_sensors,)))\n",
        "model_cnn.add(Conv1D(50, 1, activation='relu', input_shape=(1,num_sensors),padding='same'))\n",
        "model_cnn.add(Conv1D(50, 1, activation='relu'))\n",
        "model_cnn.add(MaxPooling1D(1,2))\n",
        "model_cnn.add(Dense(50,activation='relu'))\n",
        "model_cnn.add(Dense(50,activation='relu'))\n",
        "model_cnn.add(GlobalAveragePooling1D())\n",
        "model_cnn.add(Dense(1, activation='sigmoid'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9H_xeJFblrpC",
        "cellView": "form"
      },
      "source": [
        "#@title RESNET  { form-width: \"30%\" }\n",
        "n_feature_maps = 50\n",
        "\n",
        "input_layer = keras.layers.Input(num_sensors,)\n",
        "\n",
        "# BLOCK 1\n",
        "reshape = keras.layers.Reshape((1,num_sensors), input_shape=(num_sensors,))(input_layer)\n",
        "conv_x = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=1, padding='same')(reshape)\n",
        "conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "conv_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=5, padding='same')(conv_x)\n",
        "conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "conv_z = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=3, padding='same')(conv_y)\n",
        "conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "# expand channels for the sum\n",
        "shortcut_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=1, padding='same')(reshape)\n",
        "shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "\n",
        "output_block_1 = keras.layers.add([shortcut_y, conv_z])\n",
        "output_block_1 = keras.layers.Activation('relu')(output_block_1)\n",
        "\n",
        "# BLOCK 2\n",
        "\n",
        "conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_1)\n",
        "conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
        "conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
        "conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "# expand channels for the sum\n",
        "shortcut_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=1, padding='same')(output_block_1)\n",
        "shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "\n",
        "output_block_2 = keras.layers.add([shortcut_y, conv_z])\n",
        "output_block_2 = keras.layers.Activation('relu')(output_block_2)\n",
        "\n",
        "# BLOCK 3\n",
        "\n",
        "conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_2)\n",
        "conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
        "conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
        "conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "# no need to expand channels because they are equal\n",
        "shortcut_y = keras.layers.BatchNormalization()(output_block_2)\n",
        "\n",
        "output_block_3 = keras.layers.add([shortcut_y, conv_z])\n",
        "output_block_3 = keras.layers.Activation('relu')(output_block_3)\n",
        "\n",
        "# FINAL\n",
        "\n",
        "gap_layer = keras.layers.GlobalAveragePooling1D()(output_block_3)\n",
        "\n",
        "output_layer = keras.layers.Dense(1, activation='sigmoid')(gap_layer)\n",
        "\n",
        "model_rnet = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# model_m.summary()\n",
        "# from tensorflow.keras.utils import plot_model\n",
        "# plot_model(model_m, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQv63gPso2KN",
        "cellView": "form"
      },
      "source": [
        "#@title FCN  { form-width: \"30%\" }\n",
        "input_layer = keras.layers.Input(num_sensors,)\n",
        "reshape = keras.layers.Reshape((1, num_sensors), input_shape=(num_sensors,))(input_layer)\n",
        "conv1 = keras.layers.Conv1D(filters=128, kernel_size=8, padding='same')(reshape)\n",
        "conv1 = keras.layers.BatchNormalization()(conv1)\n",
        "conv1 = keras.layers.Activation(activation='relu')(conv1)\n",
        "\n",
        "conv2 = keras.layers.Conv1D(filters=256, kernel_size=5, padding='same')(conv1)\n",
        "conv2 = keras.layers.BatchNormalization()(conv2)\n",
        "conv2 = keras.layers.Activation('relu')(conv2)\n",
        "\n",
        "conv3 = keras.layers.Conv1D(128, kernel_size=3,padding='same')(conv2)\n",
        "conv3 = keras.layers.BatchNormalization()(conv3)\n",
        "conv3 = keras.layers.Activation('relu')(conv3)\n",
        "\n",
        "gap_layer = keras.layers.GlobalAveragePooling1D()(conv3)\n",
        "\n",
        "output_layer = keras.layers.Dense(1, activation='sigmoid')(gap_layer)\n",
        "\n",
        "model_fcn = keras.models.Model(inputs=input_layer, outputs=output_layer)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6MBbBxupCGh",
        "cellView": "form"
      },
      "source": [
        "#@title LSTM  { form-width: \"30%\" }\n",
        "\n",
        "from keras.regularizers import l2\n",
        "# Import Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from keras.regularizers import l2\n",
        "from time import time\n",
        "\n",
        "\n",
        "N = num_sensors                 # number of features\n",
        "EPOCH = 50                           # number of epochs\n",
        "LR = 5e-2                            # learning rate of the gradient descent\n",
        "LAMBD = 3e-2                         # lambda in L2 regularizaion\n",
        "DP = 0.0                             # dropout rate\n",
        "RDP = 0.0                            # recurrent dropout rate\n",
        "\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Reshape((1,num_sensors), input_shape=(num_sensors,)))\n",
        "model_lstm.add(LSTM(input_shape=(1,num_sensors), units=8,\n",
        "               activation='tanh', recurrent_activation='hard_sigmoid',\n",
        "               kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n",
        "               dropout=DP, recurrent_dropout=RDP,\n",
        "               return_sequences=True, return_state=False,\n",
        "               stateful=False, unroll=False\n",
        "              ))\n",
        "model_lstm.add(BatchNormalization())\n",
        "model_lstm.add(LSTM(units=8,\n",
        "               activation='tanh', recurrent_activation='hard_sigmoid',\n",
        "               kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n",
        "               dropout=DP, recurrent_dropout=RDP,\n",
        "               return_sequences=True, return_state=False,\n",
        "               stateful=False, unroll=False\n",
        "              ))\n",
        "model_lstm.add(BatchNormalization())\n",
        "model_lstm.add(LSTM(units=8,\n",
        "               activation='tanh', recurrent_activation='hard_sigmoid',\n",
        "               kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n",
        "               dropout=DP, recurrent_dropout=RDP,\n",
        "               return_sequences=False, return_state=False,\n",
        "               stateful=False, unroll=False\n",
        "              ))\n",
        "model_lstm.add(BatchNormalization())\n",
        "model_lstm.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "model_lstm.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGyeSi_mtD0L",
        "cellView": "form"
      },
      "source": [
        "#@title encoder  { form-width: \"30%\" }\n",
        "# import tensorflow.keras as keras\n",
        "# import tensorflow as tf\n",
        "# import tensorflow_addons as tfa\n",
        "# import numpy as np\n",
        "# import time\n",
        "\n",
        "# input_layer = keras.layers.Input(3,)\n",
        "# reshape = keras.layers.Reshape((1, 3), input_shape=(3,))(input_layer)\n",
        "# # conv block -1\n",
        "# conv1 = keras.layers.Conv1D(filters=128,kernel_size=5,strides=1,padding='same')(reshape)\n",
        "# conv1 = tfa.layers.InstanceNormalization()(conv1)\n",
        "# conv1 = keras.layers.PReLU(shared_axes=[1])(conv1)\n",
        "# conv1 = keras.layers.Dropout(rate=0.2)(conv1)\n",
        "# conv1 = keras.layers.MaxPooling1D(pool_size=1)(conv1)\n",
        "# # conv block -2\n",
        "# conv2 = keras.layers.Conv1D(filters=256,kernel_size=11,strides=1,padding='same')(conv1)\n",
        "# conv2 = tfa.layers.InstanceNormalization()(conv2)\n",
        "# conv2 = keras.layers.PReLU(shared_axes=[1])(conv2)\n",
        "# conv2 = keras.layers.Dropout(rate=0.2)(conv2)\n",
        "# conv2 = keras.layers.MaxPooling1D(pool_size=1)(conv2)\n",
        "# # conv block -3\n",
        "# conv3 = keras.layers.Conv1D(filters=512,kernel_size=21,strides=1,padding='same')(conv2)\n",
        "# conv3 = tfa.layers.InstanceNormalization()(conv3)\n",
        "# conv3 = keras.layers.PReLU(shared_axes=[1])(conv3)\n",
        "# conv3 = keras.layers.Dropout(rate=0.2)(conv3)\n",
        "# # split for attention\n",
        "# attention_data = keras.layers.Lambda(lambda x: x[:,:,:256])(conv3)\n",
        "# attention_softmax = keras.layers.Lambda(lambda x: x[:,:,256:])(conv3)\n",
        "# # attention mechanism\n",
        "# attention_softmax = keras.layers.Softmax()(attention_softmax)\n",
        "# multiply_layer = keras.layers.Multiply()([attention_softmax,attention_data])\n",
        "# # last layer\n",
        "# dense_layer = keras.layers.Dense(units=256,activation='sigmoid')(multiply_layer)\n",
        "# dense_layer = tfa.layers.InstanceNormalization()(dense_layer)\n",
        "# # output layer\n",
        "# flatten_layer = keras.layers.Flatten()(dense_layer)\n",
        "# output_layer = keras.layers.Dense(units=1,activation='sigmoid')(flatten_layer)\n",
        "\n",
        "# model_m = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "# model_m.summary()\n",
        "# from tensorflow.keras.utils import plot_model\n",
        "# plot_model(model_m, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFfzhxBAyBbz",
        "cellView": "form"
      },
      "source": [
        "#@title TCN\n",
        "from tcn import compiled_tcn\n",
        "from tcn import TCN, tcn_full_summary\n",
        "\n",
        "model_tcn = compiled_tcn(return_sequences=False,\n",
        "                         num_feat=1,\n",
        "                         num_classes=1,\n",
        "                         nb_filters=20,\n",
        "                         kernel_size=6,\n",
        "                         dilations=[2 ** i for i in range(9)],\n",
        "                         nb_stacks=1,\n",
        "                         max_len=num_sensors,\n",
        "                         use_weight_norm=True,\n",
        "                         use_skip_connections=True)\n",
        "model_tcn.summary()\n",
        "# from tensorflow.keras.utils import plot_model\n",
        "# plot_model(model_m, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMeHvYnlgxzc",
        "cellView": "form"
      },
      "source": [
        "#@title Model train/test  { form-width: \"30%\" }\n",
        "def trainingModel(model,tcn=False):\n",
        "  if tcn:\n",
        "    model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "                optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    # Hyper-parameters\n",
        "    BATCH_SIZE = 200\n",
        "    EPOCHS = 5\n",
        "\n",
        "    # Enable validation to use ModelCheckpoint and EarlyStopping callbacks.\n",
        "    history = model.fit(x_train,\n",
        "                          y_train,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          epochs=EPOCHS,\n",
        "\n",
        "                          validation_split=0.2,\n",
        "                          verbose=0\n",
        "                          )\n",
        "    y_pred = model.predict(x_test)\n",
        "\n",
        "    y_pred2=[]\n",
        "    for i in y_pred:\n",
        "      # print(i[0])\n",
        "      if(i[0]>i[1]):\n",
        "        y_pred2.append(i[1])\n",
        "      else:\n",
        "        y_pred2.append(i[1])\n",
        "    y_test2=[]\n",
        "    fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test,y_pred2)\n",
        "    return fpr_keras,tpr_keras\n",
        "\n",
        "  # callbacks_list = [\n",
        "  #     keras.callbacks.ModelCheckpoint(\n",
        "  #         filepath='best_model.{epoch:02d}-{val_loss:.2f}.h5',\n",
        "  #         monitor='val_loss', save_best_only=True),\n",
        "  #     keras.callbacks.EarlyStopping(monitor='accuracy', patience=1)\n",
        "  # ]\n",
        "\n",
        "  model.compile(\n",
        "      loss='binary_crossentropy',\n",
        "                  optimizer='adam', metrics=['binary_accuracy'])\n",
        "\n",
        "  # Hyper-parameters\n",
        "  BATCH_SIZE = 10\n",
        "  EPOCHS = 10\n",
        "\n",
        "  # Enable validation to use ModelCheckpoint and EarlyStopping callbacks.\n",
        "  history = model.fit(x_train,\n",
        "                        y_train,\n",
        "                        batch_size=BATCH_SIZE,\n",
        "                        epochs=EPOCHS,\n",
        "                \n",
        "                        validation_split=0.2,\n",
        "                        verbose=0\n",
        "                        )\n",
        "  accuracy_results = model.evaluate(x_test, y_test)\n",
        "  print(\"Accuracy :\",accuracy_results)\n",
        "\n",
        "  y_pred = model.predict(x_test)\n",
        "  \n",
        "  fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test,y_pred)\n",
        "  \n",
        "  return fpr_keras,tpr_keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-7ZdtmEj7h6",
        "cellView": "form"
      },
      "source": [
        "#@title Run  { form-width: \"30%\" }\n",
        "\n",
        "for i in range(len(snrDB)):\n",
        "  featuresMatrix=createFeature(beta,snrDB[i])\n",
        "  \n",
        "  df_train,df_test = train_test_split(featuresMatrix, test_size=0.2, random_state=42)\n",
        "  x_train, y_train = createTrainTest(df_train,num_sensors)\n",
        "  x_test, y_test = createTrainTest(df_test,num_sensors)\n",
        "  \n",
        "  print(snrDB[i])\n",
        "  input_shape = (num_time_periods*num_sensors)\n",
        "  x_train = x_train.reshape(x_train.shape[0], input_shape)\n",
        "  x_train = x_train.astype(\"float32\")\n",
        "  y_train = y_train.astype(\"float32\")\n",
        "\n",
        "  x_test = x_test.reshape(x_test.shape[0], input_shape)\n",
        "\n",
        "  x_test = x_test.astype(\"float32\")\n",
        "  y_test = y_test.astype(\"float32\")\n",
        "\n",
        "  fpr_cnn,tpr_cnn = trainingModel(model_cnn)\n",
        "  pd_cnn[i]=findPd(fpr_cnn,tpr_cnn)\n",
        "\n",
        "  fpr_fcn,tpr_fcn = trainingModel(model_fcn)\n",
        "  pd_fcn[i]=findPd(fpr_fcn,tpr_fcn)\n",
        "\n",
        "  fpr_lstm,tpr_lstm = trainingModel(model_lstm)\n",
        "  pd_lstm[i]=(findPd(fpr_lstm,tpr_lstm))\n",
        "\n",
        "  fpr_mlp,tpr_mlp = trainingModel(model_mlp)\n",
        "  pd_mlp[i]=(findPd(fpr_mlp,tpr_mlp))\n",
        "\n",
        "  fpr_rnet,tpr_rnet = trainingModel(model_rnet)\n",
        "  pd_rnet[i]=(findPd(fpr_rnet,tpr_rnet))\n",
        "\n",
        "  fpr_tcn,tpr_tcn = trainingModel(model_tcn,tcn=True)\n",
        "  pd_tcn[i]=(findPd(fpr_tcn,tpr_tcn))\n",
        "\n",
        "pd_cnn = savgol_filter(pd_cnn, 21, 2)\n",
        "pd_mlp = savgol_filter(pd_mlp, 21, 2)\n",
        "pd_lstm = savgol_filter(pd_lstm, 21, 2)\n",
        "pd_rnet = savgol_filter(pd_rnet, 21, 2)\n",
        "pd_tcn = savgol_filter(pd_tcn, 21, 2)\n",
        "pd_fcn = savgol_filter(pd_fcn, 21, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4PKo_4DJjYx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}